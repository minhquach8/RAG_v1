# Academic Paper Analysis with RAG Pipeline
This project processes academic PDF papers to extract key information using a Retrieval-Augmented Generation (RAG) pipeline. It leverages AI models to answer predefined questions about the papers, such as authorship, DOI, methodology, findings, and more, and saves the results in a CSV file.

## Features
- Extracts metadata (e.g., DOI, authors) from academic PDFs.
- Uses a RAG pipeline with a language model to answer questions about the papers.
- Supports progress tracking to resume processing after interruptions.
- Logs detailed information for debugging and monitoring.

## Prerequisites
- Python 3.8 or higher
- Conda (Anaconda or Miniconda)
- A Hugging Face token (stored in a `.env` file)
- PDF files stored in the specified directory 

## Installation
1. **Clone the repository**:
```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

2. **Set up the Conda environment**:Create and activate a Conda environment using the provided `environment.yml` file:
```bash
conda env create -f environment.yml
conda activate rag-pipeline
```

3. **Set up the Hugging Face token**:Create a `.env` file in the project root and add your Hugging Face token:
```bash
HF_TOKEN=your_huggingface_token
```
You can obtain a token from Hugging Face.

4. **Prepare PDF files**:Place your academic PDF files in the directory specified in `config.py`. 


## Usage

1. **Run the main script**:
```bash
python main.py
```
The program will:
- Process each PDF file in the specified directory.
- Extract metadata and answer predefined questions.
- Save results to article_answers.csv.

2. **Pause and Resume**:
- The program pauses after every 50 files for 5 minutes to cool down.
- After 200 files, it prompts you to continue or stop.
- Progress is saved in progress.txt, allowing you to resume from the last processed file.

3. **Output**:
Results are appended to `article_answers.csv` with columns for each question (e.g., `Authors_Publication`, `DOI`, `Main_Findings`).


## File Structure
```
your-repo-name/
│
├── config.py           # Configuration settings (paths, questions, model parameters)
├── utils.py            # Utility functions (logging, text cleaning, progress management)
├── pdf_processor.py    # PDF processing functions (metadata extraction, document loading)
├── rag_pipeline.py     # RAG pipeline setup and answer generation
├── main.py             # Main script to orchestrate the workflow
├── environment.yml     # Conda environment file with dependencies
```


## Dependencies
The `environment.yml` file includes all necessary packages. Key dependencies include:
- `torch`: For device support and model execution.
- `transformers`: For loading the language model.
- `langchain`: For the RAG pipeline and document processing.
- `pymupdf`: For PDF loading and metadata extraction.
- `pandas`: For saving results to CSV.

To update the `environment.yml` file after adding new dependencies:
```bash
conda env export > environment.yml
```

## Notes
- The program uses the `Qwen/Qwen3-8B` model from Hugging Face for answer generation. Ensure you have sufficient computational resources (e.g., GPU or MPS support on macOS).
- The `__pycache__` folder is auto-generated by Python for performance optimization. It is included in `.gitignore` to avoid tracking in Git.
- Logs are stored in `processing_log.txt` and refreshed every 30 minutes.

## Contributing
Contributions are welcome! Please submit a pull request or open an issue for suggestions or bug reports.